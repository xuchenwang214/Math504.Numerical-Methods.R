---
title: "HW10_504"
author: "Xuchen Wang"
date: "March 30, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
library(Matrix)
```

#### Problem 2 (d)
```{r}
########## A function help to build basis with knot#########
pos <- function(x,knot){
  # A function for basis with a knot
  x[x<=knot] <- 0
  x[x>knot] <- (x[x>knot]-knot)^3
  return(x)
}
############################################################

mass <- read.table("/Users/xuchenwang/BoneMassData.txt",header = T)[,-1]
mass <- mass[mass$gender=="female",-2]
x <- mass$age
y <- mass$spnbmd
# basis functions
h1 <- function(x)rep(1,length(x))
h2 <- function(x)x
h3 <- function(x)x^2
h4 <- function(x)x^3
h5 <- function(x)pos(x,15)
h6 <- function(x)pos(x,20)
# use normal equation to solve for coefficients
B <- cbind(h1(x),h2(x),h3(x),h4(x),h5(x),h6(x))
alpha <- solve(t(B)%*%B,t(B)%*%y)
alpha
# the function of Spline
S <- function(x){
  B <- cbind(h1(x),h2(x),h3(x),h4(x),h5(x),h6(x))
  return(B%*%alpha)
}
plot(x,y,col="blue")
curve(S,min(x),max(x),add = T,col="red")
```

S(x) =  -4.248826*h1(x)+0.9762416*h2(x)-0.07164988*h3(x)+0.001706858*h4(x)-0.002130467*h5(x)+0.0007280496*h6(x)

#### Problem 3
#### (b)
```{r}
x <- 0:10
y <- sin(x)
# new basis
h7 <- function(x)x^4
h8 <- function(x)x^5
h9 <- function(x)x^6
M <- cbind(h1(x),h2(x),h3(x),h4(x),h7(x),h8(x),h9(x))
alpha <- solve(t(M)%*%M,t(M)%*%y)
alpha
f <- function(x){
  B <- cbind(h1(x),h2(x),h3(x),h4(x),h7(x),h8(x),h9(x))
  return(B%*%alpha)
}
plot(x,y,col="blue")
curve(f,min(x),max(x),add = T,col="red")
```

f(x) = -0.0028844502+0.7953252487*x+0.5111799590*x^-0.5905649976*x^3+0.1535989224*x^4-0.0154630321*x^5+0.0005412731*x^6

#### (c)
```{r}
x <- 0:6
y <- sin(x)
# basis
M <- cbind(h1(x),h2(x),h3(x),h4(x),h7(x),h8(x),h9(x))
alpha <- solve(M,y)
alpha
f <- function(x){
  B <- cbind(h1(x),h2(x),h3(x),h4(x),h7(x),h8(x),h9(x))
  return(B%*%alpha)
}
plot(x,y,col="blue")
curve(f,min(x),max(x),add = T,col="red")
```

f(x) = 0.9037647389*x+0.2254590209*x^2-0.3576634860*x^3+0.0731892930*x^4-0.0031262599*x^5-0.0001523221*x^6

#### Problem 4 
#### (a)
```{r}
set.seed(3)
A <- matrix(rnorm(9)*10, ncol = 3)
norm <- function(x){return(sqrt(sum(x^2)))}

rayquo_iter <- function(A, start_vec=rep(1,ncol(A)), eps=10^(-6), max_i=10^4){
  # This function use Rayleigh quotient iteration to calculate a eigenvector of A
  v <- start_vec
  i <- 0
  while(TRUE){
    # Rayleigh quotient iteration
    u <- v
    lambda <- as.vector(t(u)%*%A%*%u/(t(u)%*%u))
    v <- solve(A-lambda*diag(ncol(A)),u)
    v <- v/norm(v)
    i <- i+1
    # stop condition
    if (norm(u-v)<eps || i >=max_i){break}
  }
  return(list(eigenvector=v, iterations=i))
}

eigen(A)
# check with different starting points
rayquo_iter(A,c(1,1,1))
rayquo_iter(A,c(0.5,0.5,-0.5))
rayquo_iter(A,rep(10,3))
```

#### (b)
My friend's idea will not work for PageRank. As we've seen form part(a), Rayleigh quotient iteration gives different eigenvecor when we use different starting points. In PageRank problem, we want the eigenvector whose eigenvalue equals to 1. So if we use Rayleigh quotient iteration, we must have a proper starting point whose Rayleigh quotient is approximately equal to 1, which is impossible to do intuitively. Pros: converge quickly in only a few iterations. Cons: the answer is dependent on the starting vector and therefore is not what we want sometimes.

#### (c)
```{r}
HU <- read.table("/Users/xuchenwang/edges.txt")
n <- max(HU)
HU <- data.frame(v1=c(HU$V1,1:n), v2=c(HU$V2,1:n))
obj <- graph_from_data_frame(HU)          
E <- as_adjacency_matrix(obj)     
# normalize to sum equals 1
row_pr <- 1/rowSums(E)                          # calculate non-zero probability for each row
A1 <- as.matrix(E*row_pr)                       # get the original matrix 

power_iter <- function(A, p, eps= 10^(-11), max_i= 10^4){
  # This function use power iteration to calculate the dominant eigenvector of improved matrix
  # parameter: A (original matrix)
  #            p (probability of randomly jumping)
  #            eps (error of the stop condition)
  #            max_i (maximum iterations)
  # return: v (dominant eigenvector)
  #         i (number of iterations)
  #         M (improved matrix)

  n <- ncol(A)                          # number of pages
  B <- matrix(1/n, ncol = n, nrow = n)  # matrix with uniform pr
  M <- (1-p)*A + p*B                    # improved matrix M 
  # power iteration
  set.seed(1)
  v <- rnorm(n)                         # randomly generate an initial vector
  i <- 0
  while(TRUE){
    u <- v
    v <- solve(t(M)-1.1*diag(n),u)                               
    v <- v/sum(v)                      # apply equation and normalize 
    i <- i+1
    if (norm(v-u)< eps || i >= max_i)
      {break}                          # meet stop condition, then jump out of loop
    }
  return(list(domi_vector=v, iterations=i, M=M))
}

outcome <- power_iter(A1,p=.15)
vec <- outcome$domi_vector
max <- order(vec, decreasing = T)[1:3]
max
colnames(A1)[max]
```

It turns out the same answer. But it is much slower than power iteration. I think the reason is that the shifted power iteration needs to calculate the inverse of matrix, which needs o(n^3/3) multiplications. And in power iteration, we only need n^2 multiplications in one matrix multiplication. So when n is very large, it could cause a speed problem. For the Notre Damde dataset, I think this idea does not work, since it cannot generate a sparse matrix for M-1.1*I, and therefore there is problem with inverse.












