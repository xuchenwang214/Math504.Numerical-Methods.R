---
title: "HW8_504"
author: "Xuchen Wang"
date: "March 16, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
library(Matrix)
```

#### Problem 2 (a)
```{r}
life <- read.table("/Users/xuchenwang/Desktop/US Life Expectancy 2003.txt", header = T, 
                   na.strings = NULL)
life <- life[-c(1:40),]           # Since the man is 40 years old, drop data in Age 1 to 40.
px <- 1-life$qx                   # calculate the probability of survive
L <- rep(NA,61)                   # years from 40 to 100 is 61.
L[1] <- 1                         # use a for loop to calculate L(t) for t = 0, 1, 2, ...
for (i in 2:61){
  L[i] <- L[i-1]*px[i-1]
}
t <- 0:60
L_spline <- spline(t, L, method = "natural", xout = seq(0,60,by=1/12))                                                     # use cubic spline to interpolate
plot(t, L, xlab = "t: years past 40", ylab = "probability of living past 40+t",
     main = "Plot of L(t) using cubic spline interpolation")
lines(L_spline,col="red")         
```

#### (b)
```{r}
# choose cutoff of 60*12= 720
i <- 1:720
sum(200*L_spline$y[-1]*exp(-0.05*i/12))
```

#### Problem 3 (a)
```{r}
test <- read.table("/Users/xuchenwang/Desktop/test.txt")
obj <- graph_from_data_frame(test)                  
E <- as_adjacency_matrix(obj)     
# normalize to sum equals 1
row_pr <- 1/rowSums(E)                # calculate non-zero probability for each row
A <- E*row_pr                         # get the original matrix 

power_iter <- function(A, p, eps= 10^(-11), max_i= 10^4){
  # This function use power iteration to calculate the dominant eigenvector of improved matrix
  # parameter: A (original matrix)
  #            p (probability of randomly jumping)
  #            eps (error of the stop condition)
  #            max_i (maximum iterations)
  # return: v (dominant eigenvector)
  #         i (number of iterations)
  #         M (improved matrix)

  n <- ncol(A)                          # number of pages
  B <- matrix(1/n, ncol = n, nrow = n)  # matrix with uniform pr
  M <- (1-p)*A + p*B                    # improved matrix M 
  # power iteration
  set.seed(1)
  v <- rnorm(n)                         # randomly generate an initial vector
  i <- 0
  sign <- TRUE                          # use sign to control the process of loop
  while(sign){
    u <- v
    v <- t(M)%*%v                               
    v <- v/sum(v)                      # apply equation and normalize 
    i <- i+1
    if (norm(v-u)< eps || i >= max_i)
      {sign <- FALSE}                   # meet stop condition, then jump out of loop
    }
  return(list(domi_vector=v, iterations=i, M=M))
}

p <- c(0.2,0.3,0.5,0.6,0.8,0.9)
for (i in 1:length(p)){
  out <- power_iter(A, p[i])
  M <- out$M
  lambda <- abs(eigen(M)$values)
  lambda1 <- max(lambda)                      
  lambda2 <- max(lambda[lambda!=lambda1])
  ratio <- abs(lambda2)/abs(lambda1)
  print(list(p=p[i], ratio=ratio, iterations=out$iterations))
}
```

As observed by the outcome, as p increases from 0 to 1, the ratio of lambda2/lambda1 (which is simply related to the distance) decreases (so the distance increases) and the number of iterations decreases, which demonstrates the broader the gap between lambda2 and lambda1, the faster the process runs.

#### (b)
```{r}
HU <- read.table("/Users/xuchenwang/edges.txt")
n <- max(HU)
HU <- data.frame(v1=c(HU$V1,1:n), v2=c(HU$V2,1:n))
obj <- graph_from_data_frame(HU)          
E <- as_adjacency_matrix(obj)     
# normalize to sum equals 1
row_pr <- 1/rowSums(E)                          # calculate non-zero probability for each row
A1 <- as.matrix(E*row_pr)                       # get the original matrix 

outcome <- power_iter(A1,p=.15)
vec <- outcome$domi_vector
max <- order(vec, decreasing = T)[1:3]
max
colnames(A1)[max]
```
The index of highest web pages are 2826, 934, 2918. The real number of web pages are 73, 2, 593. I tried eigen(outcome$M). It takes a long time, so I stopped it.

#### Problem 4 (a)
```{r}
notre <- read.table("/Users/xuchenwang/Desktop/NotreDame.txt")
n <- max(notre)
notre <- data.frame(v1=c(notre$V1,0:n), v2=c(notre$V2,0:n))
obj <- graph_from_data_frame(notre)          
E <- as_adjacency_matrix(obj, sparse = T)     
# normalize to sum equals 1
row_pr <- 1/rowSums(E)                # calculate non-zero probability for each row
A2 <- E*row_pr                        # get the original matrix 
```
I tried to convert this sparse matrix to normal matrix use as.matrix(A2). It shows en error: "Error in asMethod(object) : Cholmod error 'problem too large' at file ../Core/cholmod_dense.c, line 105"

```{r}
power_iter2 <- function(A, p, eps= 10^(-11), max_i= 10^4){
  # This function use power iteration to calculate the dominant eigenvector of improved matrix
  # parameter: A (original matrix)
  #            p (probability of randomly jumping)
  #            eps (error of the stop condition)
  #            max_i (maximum iterations)
  # return: v (dominant eigenvector)
  #         i (number of iterations)
  #         M (improved matrix)

  n <- ncol(A)                          # number of pages
  # power iteration
  set.seed(1)
  v <- rnorm(n)                         # randomly generate an initial vector
  i <- 0
  sign <- TRUE                          # use sign to control the process of loop
  while(sign){
    u <- v
    v <- (1-p)*t(A)%*%v + p*rep(sum(v)/n,n) 
                                       # rewrite p*t(B)*v= p*(sum(v)/n,sum(v)/n,...)
    v <- v/sum(v)                      # apply equation and normalize 
    i <- i+1
    if (norm(v-u) < eps || i >= max_i)
      {sign <- FALSE}                   # meet stop condition, then jump out of loop
    }
  return(list(domi_vector=v, iterations=i, M=M))
}

outcome1 <- power_iter2(A2, p=.15)
vec1 <- as.vector(outcome1$domi_vector)
max <- order(vec1, decreasing = T)[1:3]
max   
colnames(A2)[max]
```


