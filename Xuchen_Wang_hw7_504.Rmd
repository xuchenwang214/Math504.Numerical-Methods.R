---
title: "HW7_504"
author: "Xuchen Wang"
date: "March 2, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(erro = TRUE)
```

```{r}
source("/Users/xuchenwang/NIST.R")
```

#### Problem 2
##### (c) Write an R function GramSchmidt(A) which returns the matrix Q in (a). Check that your function works and compare to the result of using Râ€™s qr function for some nontrivial choice of A.
```{r}
norm <- function(x){
  sqrt(sum(x^2))
}
GramSchmidt <- function(A){
  n <- ncol(A)
  Q <- matrix(0, ncol = n, nrow = nrow(A))
  Q[,1] <- A[,1]/norm(A[,1])
  for (j in 2:n){
    Q[,j] <- A[,j]
    for (i in 1:(j-1))  { Q[,j] <- Q[,j] - (A[,j]%*%Q[,i])*Q[,i]}
    Q[,j] <- Q[,j]/norm(Q[,j])
  }
  print(Q)
}

A <- matrix(c(2,3,4,9), nrow = 2, ncol = 2)
B <- matrix(c(3,5,6,73,54,54,2,2,4), ncol = 3)
GramSchmidt(A)
qr.Q(qr(A))
GramSchmidt(B)
qr.Q(qr(B))
```

The first vector of result of qr function is always the negative of that of the result of GramSchmidt. The other vectors are the same. 

#### Problem 3
##### (a) 
```{r}
data <- read.table("/Users/xuchenwang/economic_data.txt",header = T)   
M <- cbind(1,as.matrix(data)[,2:7])  # design matrix
M <- unname(M)
kappa(M)
kappa(t(M)%*%M)
Q <- qr.Q(qr(M))
R <- qr.R(qr(M))
list(Q=Q, R=R)
kappa(R)
```

From the outcome, the condition number of R is much smaller than that of t(M)M. So if we apply the normal equation using R's solve function, since the condition number of t(M)M is much bigger than 10^16, it will return an error because of inaccuarte result generated by R. Instead, we could use QR decomposition to generate another equation since the condition number of R is much smaller than 10^16.

##### (b) 
```{r}
alpha <- solve(R, t(Q)%*%data$B)
lm.fit <- lm(data$B~data$A1+data$A2+data$A3+data$A4+data$A5+data$A6) 
list(alpha=alpha, alpha_lm=coef(lm.fit))
```

The solution of both method is the same.

#### Problem 4
##### (a)
```{r}
data <- read.table("/Users/xuchenwang/non_linear.txt", header = T)
y <- data$y
x <- data$x
plot(x,y)
```

##### (b)
```{r}
L <- function(b){
  r <- y - b[1]*exp(-b[2]*x) - b[3]*exp(-(x-b[4])^2/b[5]^2) -   
    b[6]*exp(-(x-b[7])^2/b[8]^2)
  sum(r^2)
}

grad_L <- function(b){
  r <- b[1]*exp(-b[2]*x) + b[3]*exp(-(x-b[4])^2/b[5]^2) + 
    b[6]*exp(-(x-b[7])^2/b[8]^2) - y
  db <- rep(NA,8)
  db[1:2] <- apply(2*r*grad_exp(b[1],b[2]), 2, sum)
  db[3:5] <- apply(2*r*grad_gau(b[3],b[4],b[5]), 2, sum)
  db[6:8] <- apply(2*r*grad_gau(b[6],b[7],b[8]), 2, sum)
  return(db)
}

##################################
# helper functions to compute the gradient
grad_exp <- function(b1,b2){
  term <- exp(-b2*x)
  return(matrix(c(term, -b1*x*term), ncol = 2))
}
grad_gau <- function(b3,b4,b5){
  term <- exp(-(x-b4)^2/b5^2)
  return(matrix(c(term, term*2*b3*(x-b4)/b5^2, term*2*b3*(x-b4)^2/b5^3), ncol = 3))
}


hessian_L <- function(x, y, b)
{
  pred_y <- get_predictions(x, b)
  nsamples <- length(x)
  
  total <- 0
  for (i in 1:nsamples) {
    c_grad <- grad.f(x[i], b)
    c_hess <- hessian.f(x[i], b)
    
    total <- total + 2 * c_grad %*% t(c_grad)
    total <- total - 2 * (y[i] - pred_y[i]) * c_hess
  }
  
  return (total)
}
##################################
# helper functions to compute the Hessian
norm <- function(z)
{
  return (sqrt(sum(z^2)))
}

# Given x values, get the predicted y values given b
get_predictions  <- function(x, b)
{
  n_samples <- length(x)
  pred_y <- rep(NA, n_samples)
  for (i in 1:n_samples)
    pred_y[i] <- f(x[i], b)
  
  return (pred_y)
}
```

##### (c)
```{r}
H <- hessian_L(x,y,1:8)
eigen(H)$values
```

Since there are negative eigenvalues, the Hessian matrix of L is not definite positive at point (1,2,3,4,5,6,7,8). Thus, function L is not convex.

##### (d)
##### i
```{r}
v1 <- c(96,0.009,103,106,18,72,151,18)
names(v1) <- paste("b",1:8, sep = "")
v2 <- c(96,0.009,103,106,1000,72,151,1000)
names(v2) <- paste("b",1:8, sep = "")
nw_multi <- function(jf, hf, start_vec, eps = 10^-4, max_i = 10^5){
  x <- start_vec
  i <- 0
  while(norm(jf(x)) > eps & i < max_i){
    x <- x - solve(hf(data$x,data$y,x),jf(x))   # newton equation
    i <- i + 1
  }
  return(list(min = x, iterations = i))
}
nw_multi(grad_L, hessian_L, v1)
```
##### nw_multi(grad_L, hessian_L, v2)
##### Error in solve.default(hf(data$x, data$y, x), jf(x)) : system is computationally singular: reciprocal condition number = 1.15483e-16
##### (note: since occurence of error, this code cannot be written in the chunk)

##### ii
```{r}
nw_multi_mod <- function(f, jf, hf, start_vec, eps = 10^-4, max_i = 10^5){
  # This function uses Newton's method with backtracking and Hessian 
  # modification to minimize the negative log likelihood.
  # parameter: f (function)
  #            jf (jacobian function)
  #            hf (hessian function)
  #            start_vec (start vector) 
  #            epsilon (limition of accuarcy)
  #            max_i (max number of iteritions)
  # return: x (minimum of function)
  #         i (the number of iterations)
  
  x <- start_vec                      # assign the start vector to x
  n <- length(x)
  i<- 0                               # initial the number of iteration to 0
  
  while(norm(jf(x)) > eps & i < max_i){
  # check if the Hessian is definite positive
  H <- hf(data$x,data$y,x)
  lambda <- min(eigen(H)$values)
  # if the Hessian is not definite positive, use Hessian modification
  if(lambda < 0){
    H <- H + (1+abs(lambda))*diag(n)   # the Hessian modification
  }
  
  # check the condition number of hessian/ modification hessian matrix
  # if the condition number is greater than 10^10, mutiply it by 2 to get a smaller
  # condition number
  
  while(kappa(H)>10^16){
    lambda <- lambda*2
    H <- H + (1+abs(lambda))*diag(n)
  }
  
  # use Newton's method with backtracking
    s <- 1                             # initial the step size to 1
    d <- -solve(H,jf(x))               # Newton's method's direction
    while(f(x+s*d)>=f(x)){s <- s/2}    # backtrack of step size
    x <- x + s*d                       # Newton's method equation
    i <- i + 1
  }
  return(list(min = x, iterations = i))
}
nw_multi_mod(L, grad_L, hessian_L, v1)
nw_multi_mod(L, grad_L, hessian_L, v2)
```

##### iii
```{r}
nls(y~ b1*exp(-b2*x) + b3*exp(-(x-b4)^2/b5^2) + b6*exp(-(x-b7)^2/b8^2), 
    data, start = v1)
```
##### nls(y~ b1*exp(-b2*x) + b3*exp(-(x-b4)^2/b5^2) + b6*exp(-(x-b7)^2/b8^2), data, start = v2)
##### Error during wrapup: singular gradient matrix at initial parameter estimates
##### (note: since occurence of error, this code cannot be written in the chunk and it loses some operations in the formula)
Newton's method with backtracking and Hessian modification is the most effective. Because for some starting points like (96,0.009,103,106,18,72,151,18), it has the same effeciency as normal Newton's method and all three method could get the same answer. But for some starting points like (96,0.009,103,106,1000,72,151,1000) whose Hessian matrix might have big condition number at some points, neither newton's method and nls function fail to get an answer while Newton's method with backtracking and Hessian modification can attain a local min in rather small number of iterations.




